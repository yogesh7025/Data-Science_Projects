{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yogesh7025/Data-Science_Projects/blob/main-packge/M1_MP2_NB_Resume_Classification_Using_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOQv69UdcrIK"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project: Resume Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNCzT95pcrIN"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMBrLtAtcrIO"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* perform data preprocessing, EDA and feature extraction on the Resume dataset\n",
        "* perform multinomial Naive Bayes classification on the Resume dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6Xda9CM9el9"
      },
      "source": [
        "### Dataset description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeRfk4Sb9h0r"
      },
      "source": [
        "The data is in CSV format, with two features: Category, and Resume.\n",
        "\n",
        "**Category** -  Industry sector to which the resume belongs to, and\n",
        "\n",
        "**Resume** - The complete CV (text) of the candidate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7ZV5ryMP07f"
      },
      "source": [
        "##  Grading = 10 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pped-3NPaDV"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuxYf07d62Oo"
      },
      "source": [
        "Companies often receive thousands of resumes for each job posting and employ dedicated screening officers to screen qualified candidates. Finding suitable candidates for an open role from a database of 1000s of resumes can be a tough task. Automated resume categorization can speeden the candidate selection process. Such automation can really ease the tedious process of fair screening and shortlisting the right candidates and aid quick decisionmaking.\n",
        "\n",
        "To learn more about this, click [here](https://www.sciencedirect.com/science/article/pii/S187705092030750X)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t0FQjPs8o4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4ba3ad-e802-4e7d-aaa6-caa98fd6786f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "standing-zimbabwe"
      },
      "source": [
        "#### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "universal-jonathan",
        "scrolled": true,
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b69252-edd4-4ea7-d2a9-369fc69778f1"
      },
      "source": [
        "#@title Download the dataset\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/Datasets/UpdatedResumeDataSet.csv\n",
        "print(\"Data Downloaded Successfuly!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Downloaded Successfuly!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3FkLI6wcaat"
      },
      "source": [
        "**Exercise 1: Read the UpdatedResumeDataset.csv dataset [0.5 Mark]**\n",
        "\n",
        "**Hint:** pd.read_csv()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIu-AOsB9GfD"
      },
      "source": [
        "# read the dataset\n",
        "# YOUR CODE HERE\n",
        "df = pd.read_csv('UpdatedResumeDataSet.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvddL7X69NiB"
      },
      "source": [
        "### Pre-processing and EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA1d25HrzTGW"
      },
      "source": [
        "**Exercise 2: Display  all the categories of resumes and their counts in the dataset [0.5 Mark]**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C92ji6ZV9MWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4e4eb5-ec2b-4d93-aebc-4882cea67cd2"
      },
      "source": [
        "# Display the distinct categories of resume\n",
        "# YOUR CODE HERE\n",
        "df['Category'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Data Science', 'HR', 'Advocate', 'Arts', 'Web Designing',\n",
              "       'Mechanical Engineer', 'Sales', 'Health and fitness',\n",
              "       'Civil Engineer', 'Java Developer', 'Business Analyst',\n",
              "       'SAP Developer', 'Automation Testing', 'Electrical Engineering',\n",
              "       'Operations Manager', 'Python Developer', 'DevOps Engineer',\n",
              "       'Network Security Engineer', 'PMO', 'Database', 'Hadoop',\n",
              "       'ETL Developer', 'DotNet Developer', 'Blockchain', 'Testing'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtIjY7ji9va5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e60b00-92d3-4052-c7c5-d2e572fd5a1c"
      },
      "source": [
        "# Display the distinct categories of resume and the number of records belonging to each category\n",
        "# YOUR CODE HERE\n",
        "df['Category'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Java Developer               84\n",
              "Testing                      70\n",
              "DevOps Engineer              55\n",
              "Python Developer             48\n",
              "Web Designing                45\n",
              "HR                           44\n",
              "Hadoop                       42\n",
              "Blockchain                   40\n",
              "ETL Developer                40\n",
              "Operations Manager           40\n",
              "Data Science                 40\n",
              "Sales                        40\n",
              "Mechanical Engineer          40\n",
              "Arts                         36\n",
              "Database                     33\n",
              "Electrical Engineering       30\n",
              "Health and fitness           30\n",
              "PMO                          30\n",
              "Business Analyst             28\n",
              "DotNet Developer             28\n",
              "Automation Testing           26\n",
              "Network Security Engineer    25\n",
              "SAP Developer                24\n",
              "Civil Engineer               24\n",
              "Advocate                     20\n",
              "Name: Category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpHv50ojzvO5"
      },
      "source": [
        "**Exercise 3: Create the count plot of different categories [0.5 Mark]**\n",
        "\n",
        "**Hint:** Use `sns.countplot()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYwrK_5f93gP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7f51fa4a-7e32-4ce1-818a-2d2ae76efd2a"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "plt.figure(figsize=(25,3))\n",
        "sns.countplot(df)\n",
        "plt.tight_layout()\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend().set_visible(False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-96bb2f289305>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=df.reset_index(), x=df['Category'].uni, y='count')"
      ],
      "metadata": {
        "id": "TpaTV2V9RvLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Category'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy1JSSNUR4MH",
        "outputId": "ea3641b2-cf53-4935-e0b7-a256c521f64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Java Developer               84\n",
              "Testing                      70\n",
              "DevOps Engineer              55\n",
              "Python Developer             48\n",
              "Web Designing                45\n",
              "HR                           44\n",
              "Hadoop                       42\n",
              "Blockchain                   40\n",
              "ETL Developer                40\n",
              "Operations Manager           40\n",
              "Data Science                 40\n",
              "Sales                        40\n",
              "Mechanical Engineer          40\n",
              "Arts                         36\n",
              "Database                     33\n",
              "Electrical Engineering       30\n",
              "Health and fitness           30\n",
              "PMO                          30\n",
              "Business Analyst             28\n",
              "DotNet Developer             28\n",
              "Automation Testing           26\n",
              "Network Security Engineer    25\n",
              "SAP Developer                24\n",
              "Civil Engineer               24\n",
              "Advocate                     20\n",
              "Name: Category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyXtz0Mr0NVB"
      },
      "source": [
        "**Exercise 4: Create a pie plot depicting the percentage of resume distributions category-wise [0.5 mark]**\n",
        "\n",
        "**Hint:** Use [plt.pie()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pie.html) and [plt.get_cmap](https://matplotlib.org/stable/tutorials/colors/colormaps.html) for color mapping the pie chart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrpJDoGx-CuF"
      },
      "source": [
        "targetCounts = df['Category'].value_counts()\n",
        "targetLabels  = df['Category'].unique()\n",
        "# Make square figures and axes\n",
        "plt.figure(1, figsize=(25,25))\n",
        "the_grid = GridSpec(2, 2)\n",
        "\n",
        "# YOUR CODE HERE to display pie chart with color coding (eg. `coolwarm`)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJaznteJ1xr4"
      },
      "source": [
        "**Exercise 5: Convert all the `Resume` text to lower case [0.5 Mark]**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf1wNRqb-Om2"
      },
      "source": [
        "# Convert all characters to lowercase\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cBXmdXpDIQJ"
      },
      "source": [
        "### Cleaning resumes' text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvkRbRnM3ap7"
      },
      "source": [
        "**Exercise 6: Define a function to clean the resume text [2 Mark]**\n",
        "\n",
        "In the text there are special characters, urls, hashtags, mentions, etc. Remove the following:  \n",
        "\n",
        "* URLs: For reference click [here](https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python)\n",
        "* RT | cc: For reference click [here](https://www.machinelearningplus.com/python/python-regex-tutorial-examples/)\n",
        "* Hashtags, # and Mentions, @\n",
        "* punctuations\n",
        "* extra whitespace\n",
        "\n",
        "PS: Use the provided reference similarly for removing any other such elements.\n",
        "\n",
        "After cleaning as above, store the Resume Text in a separate column (New Feature).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Z-Oois_LWE"
      },
      "source": [
        "import re\n",
        "def cleanResume(resumeText):\n",
        "  # YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O2tR_IjDNWr"
      },
      "source": [
        " # apply the function defined above and save the\n",
        " # YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EnSuTubI_S1"
      },
      "source": [
        "sent_lens = []\n",
        "for i in df.cleaned_resume:\n",
        "    length = len(i.split())\n",
        "    sent_lens.append(length)\n",
        "\n",
        "print(len(sent_lens))\n",
        "print(max(sent_lens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktaCkU5yG5V0"
      },
      "source": [
        "### Stopwords removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0XDeidJG-hc"
      },
      "source": [
        "The stopwords, for example, `and, the, was, and so forth` etc. appear very frequently in the text and are not helpful in the predictive process. Therefore these are usually removed for text analytics and text classification purposes.\n",
        "\n",
        "1. Tokenize the input words into individual tokens and store it in an array\n",
        "2. Using `nltk.corpus.stopwords`, remove the stopwords\n",
        "\n",
        "Hint: See Module 1 - Assignment 4 'Text Classification using Naive Bayes'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWP5cZL5_f0V"
      },
      "source": [
        "**Exercise 7: Use `nltk` package to find the most common words from the `cleaned resume` column [2 Marks]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRZSRsUT_G7h"
      },
      "source": [
        "**Hint:**\n",
        "* Use `nltk.FreqDist`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvSiTsqqXjeT"
      },
      "source": [
        "# stop words\n",
        "# YOUR CODE HERE to print the stop words in english language"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v9bZeMwX0lN"
      },
      "source": [
        "# most common words\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYAJU8teYBlv"
      },
      "source": [
        "# YOUR CODE HERE to show the most common word using WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpxqmsM6TmJu"
      },
      "source": [
        "**Exercise 8: Convert the categorical variable `Category` to a numerical feature and make a different column, which can be treated as the target variable [0.5 Mark]**\n",
        "\n",
        "**Hint:** Use [`sklearn.preprocessing.LabelEncoder()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg4QNr7DYSJ5"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUauwAsOn31f"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTTiMQvYA77m"
      },
      "source": [
        "**Exercise 9: Convert the text to feature vectors by applying `tfidf vectorizer` to the Label encoded category made above [2 Marks]**\n",
        "\n",
        "`TF-IDF`will tokenize documents, learn the vocabulary, inverse document frequency weightings, and allow you to encode new documents\n",
        "\n",
        "**Hint:** Use [`TfidfVectorizer()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUsZIUQyY_wX"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPTb3I-Oon2D"
      },
      "source": [
        "## Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocjoqFlCFJn3"
      },
      "source": [
        "**Exercise 10: Split the data into train and test sets. Apply Naive Bayes Classifier (MultinomialNB) and evaluate the model predictions [1 mark]**\n",
        "\n",
        "**Hint:** Use Vectorized features made above as X and Labelled category as y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYoRwtlWobMI"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr0xI6QwFN-Q"
      },
      "source": [
        "## Optional: Create a Gradio based web interface to test and display the model predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "349aUvoInpJv"
      },
      "source": [
        "**Report Analysis**\n",
        "- Which method(s), other than TF-IDF could be used for text to vector conversion?\n",
        "- Discuss about the `alpha`, `class_prior` and `fit_prior` parameters in sklearn `MultinomialNB`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several methods other than TF-IDF (Term Frequency-Inverse Document Frequency) that can be used for text-to-vector conversion in natural language processing (NLP). Some of the common methods include:\n",
        "\n",
        "1. **Bag of Words (BoW)**: BoW represents text as a vector where each dimension corresponds to a unique word in the entire corpus. Each element in the vector counts the frequency of the corresponding word in the text, regardless of word order or context.\n",
        "\n",
        "2. **Word Embeddings (Word2Vec, GloVe)**: Word embeddings are dense vector representations of words that capture semantic relationships between words. These embeddings are pre-trained on large text corpora and can be used to represent words, phrases, or documents as vectors. Popular methods include Word2Vec and GloVe.\n",
        "\n",
        "3. **Doc2Vec**: Doc2Vec is an extension of Word2Vec that can represent entire documents as continuous-valued vectors. It assigns a unique vector to each document, capturing its semantic meaning.\n",
        "\n",
        "4. **FastText**: FastText is an extension of Word2Vec that can represent words as subword embeddings. This is particularly useful for handling out-of-vocabulary words and morphological variations.\n",
        "\n",
        "5. **Latent Semantic Analysis (LSA)**: LSA is a dimensionality reduction technique that applies singular value decomposition (SVD) to a term-document matrix to capture latent semantic relationships between terms and documents. It's used to reduce the dimensionality of the data.\n",
        "\n",
        "6. **Word Frequency Counts**: Instead of TF-IDF, you can use simple word frequency counts where each dimension in the vector represents the count of a specific word in the text.\n",
        "\n",
        "7. **One-Hot Encoding**: Each word in a text is represented by a binary vector where only one element is 1 (indicating the presence of that word) and all others are 0s.\n",
        "\n",
        "8. **Hashing Vectorizer**: Hashing vectorization is a memory-efficient way of converting text to vectors by using a fixed-size hash space. It hashes words to specific dimensions in the vector, reducing the dimensionality.\n",
        "\n",
        "9. **Paragraph Vectors (Doc2Vec)**: Similar to Doc2Vec, paragraph vectors represent documents by learning a vector representation for each document based on the words it contains.\n",
        "\n",
        "10. **BERT Embeddings**: Bidirectional Encoder Representations from Transformers (BERT) embeddings are pre-trained contextual word embeddings that capture the meaning of words in the context of surrounding words. Fine-tuning BERT on specific tasks is also common.\n",
        "\n",
        "The choice of method depends on the specific NLP task and the characteristics of your data. TF-IDF, Word2Vec, and BERT embeddings are among the most popular methods, but the suitability of each method should be determined based on the problem you are trying to solve."
      ],
      "metadata": {
        "id": "Ps_dnrS-eqLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In scikit-learn's `MultinomialNB` (Multinomial Naive Bayes) classifier, there are several important parameters, including `alpha`, `class_prior`, and `fit_prior`, which influence how the model is trained and how it makes predictions in the context of a multinomial distribution. Here's an explanation of each parameter:\n",
        "\n",
        "1. **alpha**:\n",
        "   - `alpha` is a smoothing hyperparameter used to handle the problem of zero probabilities. It's also known as Laplace smoothing or add-one smoothing.\n",
        "   - Laplace smoothing is applied to avoid zero probabilities when a feature (word or term) in the test data has not been seen in the training data for a particular class. It adds a small positive value (alpha) to the counts of each feature in each class.\n",
        "   - A smaller alpha value results in less smoothing, which means the model will rely more on the observed data. A larger alpha value introduces more smoothing, making the model less sensitive to rare features.\n",
        "   - The default value for `alpha` is 1.0, which corresponds to Laplace smoothing. You can adjust it based on the characteristics of your data and the problem you're trying to solve.\n",
        "\n",
        "2. **class_prior**:\n",
        "   - `class_prior` is an optional parameter that allows you to specify prior probabilities for each class in the target variable. It is a list of floats representing the prior probabilities for each class.\n",
        "   - If you don't specify `class_prior`, the classifier will estimate the class prior probabilities from the training data.\n",
        "   - Specifying `class_prior` can be useful when you want to give more weight to certain classes or when the distribution of classes in your training data does not accurately represent the real-world distribution.\n",
        "\n",
        "3. **fit_prior**:\n",
        "   - `fit_prior` is a Boolean parameter that determines whether the model should estimate class prior probabilities from the training data (`fit_prior=True`) or whether it should use the provided `class_prior` values (`fit_prior=False`).\n",
        "   - If `fit_prior=True`, the classifier will estimate class prior probabilities based on the relative frequencies of classes in the training data.\n",
        "   - If `fit_prior=False`, the classifier will use the `class_prior` values that you provide.\n",
        "   - Setting `fit_prior=False` can be useful when you have prior knowledge about class probabilities that you want to use, or when you want to maintain consistency in class prior probabilities across different runs of the classifier.\n",
        "\n",
        "In summary, `alpha` controls the amount of smoothing applied to probabilities, `class_prior` allows you to specify custom class prior probabilities, and `fit_prior` determines whether the model estimates class prior probabilities from the data or uses the provided values. These parameters provide flexibility in how you configure your Multinomial Naive Bayes classifier based on the specifics of your classification problem."
      ],
      "metadata": {
        "id": "9QX12ve5ejSX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnFxtl619jg_"
      },
      "source": [
        "Dataset Source Reference: [Resume dataset](https://www.kaggle.com/gauravduttakiit/resume-dataset/download)"
      ]
    }
  ]
}